{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "- **Probabilistic classifier**\n",
    "- Based on Bayes' Theorem with the assumption that features are independent given the class. It computes the probability of each class given the feature values and classifies based on the highest probability.\n",
    "- **Assumptions**:\n",
    "  - Features are conditionally independent given the class label. This is rarely true in real-world data.\n",
    "  - Assumes class conditional independence, meaning each feature contributes independently to the probability of a class label.\n",
    "  \n",
    "**Bayes' Theorem:**\n",
    "\n",
    "$P(C_k \\mid x) = \\dfrac{P(x \\mid C_k) \\cdot P(C_k)}{P(x)}$\n",
    "\n",
    "**For Gaussian Naive Bayes (continuous features):**\n",
    "\n",
    "$P(x_i \\mid C_k) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\exp \\left( - \\dfrac{(x_i - \\mu_k)^2}{2 \\sigma_k^2} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "- **Linear classifier**\n",
    "- Single-layered neural network model that classifies data by finding a linear boundary. The Perceptron updates weights based on misclassified examples, iterating over the data until convergence or a set number of iterations.\n",
    "- **Assumptions**:\n",
    "  - Assumes that the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the classes. If this isn't true, the Perceptron will not converge.\n",
    "  - Typically used for binary classification, although multi-class versions exist.\n",
    "  - Assumes no significant noise.\n",
    "\n",
    "**Perceptron Update Rule:**\n",
    "\n",
    "$w \\leftarrow w + \\eta \\cdot (y - \\hat{y}) \\cdot x$\n",
    "\n",
    "**Prediction Function (Linear):**\n",
    "\n",
    "$\\hat{y} = \\text{sign}(w \\cdot x + b)$\n",
    "\n",
    "---\n",
    "\n",
    "### Adaline and Gradient Descent\n",
    "\n",
    "- **Linear classifier** (similar to Perceptron)\n",
    "- Uses a continuous activation function (linear output) and updates weights using gradient descent. It minimizes the sum of the squared errors (MSE) instead of the misclassification errors.\n",
    "- Can converge to a better solution due to the continuous output; useful for regression and classification.\n",
    "- **Cons**: Sensitive to learning rate.\n",
    "- **Assumptions**:\n",
    "  - Assumes a linear relationship between input features and target variables.\n",
    "  - Gradient descent works best when the errors are normally distributed.\n",
    "  - Adaline also uses the MSE, assuming a continuous error surface that can be minimized by gradient descent.\n",
    "\n",
    "**Activation Function (Linear):**\n",
    "\n",
    "$z = w \\cdot x + b$\n",
    "\n",
    "**Mean Squared Error (MSE) Cost Function:**\n",
    "\n",
    "$J(w) = \\dfrac{1}{2} \\sum (y - z)^2 = \\dfrac{1}{2} \\sum \\left( y - (w \\cdot x + b) \\right)^2$\n",
    "\n",
    "**Weight Update Rule using Gradient Descent:**\n",
    "\n",
    "$w \\leftarrow w + \\eta \\sum (y - z) \\cdot x$\n",
    "\n",
    "$b \\leftarrow b + \\eta \\sum (y - z)$\n",
    "\n",
    "---\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "- **Linear or nonlinear classifier**\n",
    "- SVM finds the optimal hyperplane that separates data points of different classes with the maximum margin. The margin is defined as the distance between the hyperplane and the closest data points, which are called support vectors.\n",
    "- SVM is effective in high-dimensional spaces and robust against overfitting.\n",
    "- **Assumptions**:\n",
    "  - Maximum margin principle: the best classification boundary is the one that maximizes the margin between the support vectors of each class.\n",
    "\n",
    "**Decision Boundary:**\n",
    "\n",
    "$f(x) = w \\cdot x + b = 0$\n",
    "\n",
    "**Optimization Objective:**\n",
    "\n",
    "$\\min_w \\dfrac{1}{2} ||w||^2 \\quad \\text{subject to} \\quad y_i(w \\cdot x_i + b) \\geq 1 \\quad \\forall i$\n",
    "\n",
    "**For Soft-margin SVM:**\n",
    "\n",
    "$\\min_w \\dfrac{1}{2} ||w||^2 + C \\sum_{i=1}^{n} \\xi_i$\n",
    "\n",
    "---\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "- **Linear classifier** (though it uses a non-linear transformation)\n",
    "- Logistic regression is used for binary classification tasks, predicting the probability that an instance belongs to a particular class. The model uses a linear combination of features and applies the sigmoid (logistic) function to map the output to a probability between 0 and 1.\n",
    "- **Assumptions**:\n",
    "  - Assumes a linear relationship between the input features and the log-odds of the target variable.\n",
    "  - Assumes that the classes are separable in terms of the log-odds, but not necessarily linearly separable in feature space.\n",
    "  - Assumes that there is little or no multicollinearity between the features.\n",
    "\n",
    "**Sigmoid Function:**\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "Where $z = w^T X + b$.\n",
    "\n",
    "**Prediction Function:**\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\sigma(w^T X + b) = \\frac{1}{1 + e^{-(w^T X + b)}}\n",
    "$$\n",
    "If the predicted probability is greater than 0.5, the model classifies the instance as class 1, otherwise class 0.\n",
    "\n",
    "**Log Loss (Cross-Entropy Loss):**\n",
    "\n",
    "The model minimizes the log loss during training:\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "Where $y_i$ is the true label and $p_i$ is the predicted probability.\n",
    "\n",
    "**Regularization (Optional):**\n",
    "\n",
    "Logistic regression can be regularized to avoid overfitting:\n",
    "- **L2 Regularization (Ridge)**: Penalizes the sum of squared coefficients.\n",
    "- **L1 Regularization (Lasso)**: Penalizes the absolute values of the coefficients, encouraging sparsity.\n",
    "- **Elastic Net**: Combines L1 and L2 regularization.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Kernel Support Vector Machines\n",
    "\n",
    "- **Nonlinear classifier**\n",
    "- Extension of SVM that allows it to handle nonlinear data. The kernel trick is used to transform data into higher dimensions without explicitly calculating the transformation, enabling SVM to find a separating hyperplane even for complex datasets.\n",
    "- Popular kernels: linear, polynomial, RBF (radial basis function)\n",
    "- **Assumptions**:\n",
    "  - Data is linearly separable in a higher-dimensional space via the kernel function.\n",
    "\n",
    "**Kernel Trick (for nonlinear data):**\n",
    "\n",
    "$f(x) = \\sum \\alpha_i y_i K(x_i, x_j) + b$\n",
    "\n",
    "**Common Kernels:**\n",
    "\n",
    "- **Linear Kernel**:\n",
    "\n",
    "  $K(x_i, x_j) = x_i \\cdot x_j$\n",
    "\n",
    "- **Polynomial Kernel**:\n",
    "\n",
    "  $K(x_i \\cdot x_j + 1)^d$\n",
    "\n",
    "- **RBF (Radial Basis Function)**:\n",
    "\n",
    "  $K(x_i, x_j) = \\exp \\left( - \\gamma ||x_i - x_j||^2 \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "- **Tree-based classifier**\n",
    "- Decision trees split data by recursively selecting features and thresholds that result in the best separation between classes. They do this by using criteria such as entropy, information gain, or Gini impurity. At each node, it makes a decision that leads to a classification at the leaf nodes.\n",
    "- **Cons**: Prone to overfitting, especially with deep trees. You can avoid this by limiting the maximum depth.\n",
    "- **Assumptions**:\n",
    "  - Assumes irrelevant features are ignored and will not be used in splits, since the splitting criteria select only the most informative features.\n",
    "  - Assumes splits can be made along one feature at a time; not ideal for data that requires non-axis-aligned boundaries.\n",
    "  - Assumes splits are made independently at each level of the tree.\n",
    "\n",
    "**Information Gain:**\n",
    "\n",
    "$\\text{IG}(D, A) = H(D) - \\sum_{v \\in A} \\dfrac{|D_v|}{|D|} H(D_v)$\n",
    "\n",
    "**Entropy:**\n",
    "\n",
    "$H(D) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)$\n",
    "\n",
    "---\n",
    "\n",
    "### K Nearest Neighbors (KNN)\n",
    "\n",
    "- **Instance-based classifier**\n",
    "- Lazy learning algorithm that doesn't build a model during training. Instead, it classifies new points by looking at the \"K\" closest training examples and choosing the most common class among them. This makes it computationally expensive at prediction time. It is also sensitive to the choice of \"k\" and requires feature scaling for better performance.\n",
    "- **Assumptions**:\n",
    "  - Local homogeneity (points closer to each other are more similar).\n",
    "  - Assumes feature importance is equal.\n",
    "  - Uses Euclidean distance, which is an assumption in itself.\n",
    "  - Feature scaling is necessary since features with larger ranges can dominate the distance calculations, violating the assumption that all features contribute equally.\n",
    "\n",
    "**Distance Metric (Euclidean Distance):**\n",
    "\n",
    "$d(x_i, x_j) = \\sqrt{ \\sum_{k=1}^{n} (x_{ik} - x_{jk})^2 }$\n",
    "\n",
    "**Classification Decision:**\n",
    "\n",
    "$\\hat{y} = \\text{mode}\\left( \\{ y_{i_1}, y_{i_2}, \\dots, y_{i_k} \\} \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Imputation\n",
    "\n",
    "Imputation is the process of filling in missing data points in a dataset. There are a few types of imputation:\n",
    "- mean/median/mode imputation\n",
    "- KNN imputation - look at closest data points based on other features values and imputing based on their values\n",
    "- Multiple imputation - generates multiple imputations for each missing value, creating multiple \"complete\" datasets that are analyzed separately, then aggregated\n",
    "\n",
    "### Ordinal vs. Nominal Features\n",
    "\n",
    "Categorical data can be classified as either ordinal or nominal, depending on whether the values have an inherent order. \n",
    "\n",
    "- Ordinal features: represent categories that have a clear, ordered relationship. \n",
    "    - example: \"low, medium, high\"\n",
    "    - They are typically transformed using ordinal encoding, where each category is replaced by its rank (low = 1, medium = 2, high = 3)\n",
    "- Nominal features: reprsent categories with no inherent order. \n",
    "    - example: \"red, blue, green\"\n",
    "    - one-hot encoding, where each cateogry is represented as a binary column (1 indicating presence, 0 indicating no presence)\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "Feature scaling is a method used to normalize the range of independent variables or features of data. It is essential when the data involves features with different units or scales, since many machine learning models are sensitive to the range of input features.\n",
    "\n",
    "There are 2 popular methods for feature scaling\n",
    "- Normalization rescales the values of a feature so that they range between 0 and 1\n",
    "- Standardization is preferred when the feature distribution is Gaussian or when there are outliers, so it doesn't squish the data into a strict 0 to 1 range\n",
    "\n",
    "Sequential Backwards Selection (SBS) is a feature selection technique used to reduce the number of input features while maintaining or improving model performance. The goal is to remove irrelevant or redundant features that do not contribute to the predictive performance of a model. \n",
    "- Basically, train the model\n",
    "- evaluate performance\n",
    "- remove feature that leads to least decrease in model performance\n",
    "- repeat until a stopping criterion is reached (significant drop in performance, etc.)\n",
    "\n",
    "SBS is a greedy algorithm, and does not explore all possible subsets of features but sequentially removes them, which makes it computationally less expensive than exhaustive methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction aims to reduce the number of features (or dimensions) in a dataset while retaining as much information as possible. This helps with simplifying models, reducing computational cost, and mitigating the curse of dimensionality, which can lead to overfitting.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an unsupervised linear dimensionality reduction technique that transforms a dataset’s features into a new set of linearly uncorrelated variables called **principal components (PCs)**. These components are ordered by the amount of variance they capture.\n",
    "\n",
    "- **Variance Maximization**: PCA projects data onto directions (PCs) that maximize variance in the dataset. The first principal component captures the most variance, the second captures the next largest, and so on.\n",
    "  \n",
    "- **Orthogonal Transformation**: The principal components are orthogonal (uncorrelated) to one another, ensuring that each component captures unique variance in the data.\n",
    "\n",
    "- **Linear Method**: PCA assumes the data can be represented in a **linear subspace**. This means it works best when the underlying structure of the data is approximately linear.\n",
    "\n",
    "- **Eigenvectors and Eigenvalues**: PCA relies on the **eigenvectors** and **eigenvalues** of the covariance matrix of the data. The eigenvectors represent the directions (principal components), and the eigenvalues measure the magnitude of variance in each direction.\n",
    "\n",
    "- **Dimensionality Selection**: You can reduce the dataset to fewer dimensions by selecting only the first few principal components that capture the majority of the variance. Typically, the number of components is selected by keeping enough components to explain, say, 95% of the variance.\n",
    "\n",
    "- **Key Equations**:\n",
    "  - Covariance matrix: $ \\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)(x_i - \\mu)^T $\n",
    "  - Eigenvalue decomposition: $ \\Sigma v = \\lambda v $\n",
    "  - Project data: $ X_{\\text{projected}} = X W_k $ (where $ W_k $ is the matrix of top $ k $ eigenvectors)\n",
    "\n",
    "### Supervised Dimensionality Reduction\n",
    "\n",
    "Supervised dimensionality reduction methods consider both the feature set (independent variables) and the target labels (dependent variables) during the reduction process. The goal is to reduce dimensionality while preserving class separability.\n",
    "\n",
    "#### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is a supervised linear technique that aims to maximize the separation between different classes in a lower-dimensional space.\n",
    "\n",
    "- **Maximizing Class Separability**: LDA projects data onto a lower-dimensional space by maximizing the **between-class variance** and minimizing the **within-class variance**. This increases the distance between class means while minimizing the spread of each class.\n",
    "\n",
    "- **Class-Specific Means and Covariance**: LDA calculates the means of each class and the overall covariance matrix to find the optimal directions for projection.\n",
    "\n",
    "- **Linear Combination of Features**: LDA finds a linear combination of features that best separates two or more classes.\n",
    "\n",
    "- **Assumptions**:\n",
    "  - Classes are normally distributed (Gaussian distributions).\n",
    "  - Classes have the same covariance matrix (homoscedasticity).\n",
    "  \n",
    "- **Key Equations**:\n",
    "  - Between-class scatter matrix: $ S_B = \\sum_{i=1}^{k} n_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T $\n",
    "  - Within-class scatter matrix: $ S_W = \\sum_{i=1}^{k} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T $\n",
    "  - LDA projection: Maximize $ \\frac{|S_B|}{|S_W|} $ to find optimal projection directions.\n",
    "\n",
    "### Nonlinear Dimensionality Reduction\n",
    "\n",
    "Linear methods like PCA and LDA assume that the data lies in a linear subspace, which often doesn't hold in real-world applications. Nonlinear dimensionality reduction techniques aim to uncover the underlying low-dimensional manifold on which the high-dimensional data resides. These methods are often used when the data has a complex structure that cannot be captured by linear transformations.\n",
    "\n",
    "#### Common Nonlinear Methods:\n",
    "\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:\n",
    "  - Projects high-dimensional data into 2D or 3D for visualization.\n",
    "  - Focuses on preserving local structure, making similar points stay close in the lower-dimensional space.\n",
    "  \n",
    "- **Isomap**:\n",
    "  - Aims to preserve the **geodesic distances** between points on a nonlinear manifold.\n",
    "  - Combines classical MDS (multidimensional scaling) with nearest-neighbor graphs.\n",
    "\n",
    "- **Kernel PCA**:\n",
    "  - Extends PCA by using the kernel trick to capture **nonlinear relationships** between features.\n",
    "  - Projects data into a higher-dimensional feature space where linear PCA is performed.\n",
    "\n",
    "- **Key Concepts**:\n",
    "  - **Manifold Learning**: Nonlinear techniques assume the data lies on a lower-dimensional manifold embedded in the higher-dimensional space.\n",
    "  - **Geodesic Distance**: The shortest path between two points on a curved surface or manifold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Models\n",
    "\n",
    "Regression models are a type of statistical model used to predict a continuous outcome variable based on one or more input features. These models estimate relationships between input and output features to predict values for new data points.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "This model assumes a linear relationship between the dependent and independent variables. It finds the line of best fit by minimizing the mean squared error (MSE). \n",
    "\n",
    "Linear regression that uses the mean-squared error as the loss function is referred to as ordinary least squares (OLS). \n",
    "\n",
    "### Ridge Regression \n",
    "\n",
    "This is a type of linear regression that includes regularization to prevent overfitting and improve the stability of the model. Specifically, ridge regression adds an L2 penalty to the loss function to shrink the model coefficients towards zero by adding a constraint on their magnitudes. This helps when the data has mluticollinearity or when the number of features is much larger than the number of observations.\n",
    "\n",
    "$$\n",
    "J(β) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_i$ is the actual value of the response variable\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "- $\\beta_j$ are the regression coefficients\n",
    "- $\\lambda$ is a hyperparameter that controls the strength of the regularization.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{β} = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "The key difference between ridge and OLS is the addition of the penalty term, which is the sum of the squared coefficients.\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation Metrics for Regression\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: Calculates the average absolute difference between the actual and predicted values. \n",
    "\n",
    "  $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "MAE represents the average magnitude of the errors without considering their direction (positive or negative). It's easy to interpret because it is in the same units as the original data.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Measures the average of the squared differences between actual and predicted values. By squaring the errors, it gives more weight to larger deviations, making it more sensitive to outliers. This metric is useful when large errors are particularly undesirable.\n",
    "\n",
    "  $\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $\n",
    "\n",
    "\n",
    "- **R-Squared (R²)**: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. R² values range from 0 to 1, where 1 indicates a perfect fit and 0 means no explanatory power.\n",
    "\n",
    "  $\n",
    "  R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "  $\n",
    "\n",
    "- **Adjusted R-Squared**: A modified version of R² that adjusts for the number of predictors in the model, helping prevent overfitting. It’s especially useful for comparing models with different numbers of predictors.\n",
    "\n",
    "  $\n",
    "  \\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n - 1}{n - p - 1}\n",
    "  $\n",
    "\n",
    "  where \\( n \\) is the number of observations, and \\( p \\) is the number of predictors.\n",
    "\n",
    "- **Mean Absolute Percentage Error (MAPE)**: Calculates the average of the absolute percentage errors, providing a relative measure of error. It’s useful for understanding error in percentage terms but can be sensitive to small values in the denominator.\n",
    "\n",
    "  $\n",
    "  \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\n",
    "  $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots\n",
    "A residual is the difference between the predicted target and the actual target. In a perfect predictive model, the residuals should all be zero. \n",
    "\n",
    "Good fit: residuals should be randomly distributed with minimal patterns. If there are patterns then that means that the model is underfitting because there is explanatory structure from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "## Hierarhical Clustering\n",
    "Hierarchical clustering algorithms group data points into clusters based on their similarity. Unlike other clustering methods like K-means, which requires a predefined number of clusters, hierarchical clustering buildsa hierarchy of clusteres and allows you to decide the number of clusteres at a later stage by cutting the dendrogram at a chosen level.\n",
    "\n",
    "1. Agglomerative (Bottom-Up)\n",
    "- Starts with each data point as its own cluster\n",
    "- Iteratively merges the clsoest clusters based on a similarity or distance metric (e.g. Euclidean distance)\n",
    "- Stops when all points are merged into a single cluster or whena specified number of clusters is reached.\n",
    "\n",
    "2. Divisive (top-down)\n",
    "- starts with all data points in a single cluster\n",
    "- iteratively splits clusters into smaller clusters until each point is its own cluster or a stopping criterion is met\n",
    "\n",
    "Steps in hierarchical clustering:\n",
    "1. Compute Distance Matrix\n",
    "2. Linkage Criteria\n",
    "- Single linkage\n",
    "- Average LInkage\n",
    "- Maximum Linkage\n",
    "- Ward's method\n",
    "3. Merge or Split Clusters\n",
    "4. Construct a Dendrogram\n",
    "\n",
    "Advantages of hierarchical: does not require number of clusteres to be predefined, produces a hierarchy of clusters, which can give more insight into the data structure, can work well with small to medium-sized datasets\n",
    "\n",
    "Disadvantages: computationally intensive for large datasets (b/c of distance matrix), sensitive to noise and outliers, which can distort the hierarchical structure, choice of distance metric and linkage method can significantly affect results\n",
    "\n",
    "## Spectral Clustering\n",
    "Spectral clustering is a graph-based clustering method that uses eigenvalues and eigenvectors of a graph's Lapalcian matrix to identify clusters. Unlike traditional clustering methods like K-means, spectral clustering excels at identifying non-convex clusters and clusters that may have irregular shapes.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
