{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Imputation\n",
    "\n",
    "Imputation is the process of filling in missing data points in a dataset. There are a few types of imputation:\n",
    "- mean/median/mode imputation\n",
    "- KNN imputation - look at closest data points based on other features values and imputing based on their values\n",
    "- Multiple imputation - generates multiple imputations for each missing value, creating multiple \"complete\" datasets that are analyzed separately, then aggregated\n",
    "\n",
    "### Ordinal vs. Nominal Features\n",
    "\n",
    "Categorical data can be classified as either ordinal or nominal, depending on whether the values have an inherent order. \n",
    "\n",
    "- Ordinal features: represent categories that have a clear, ordered relationship. \n",
    "    - example: \"low, medium, high\"\n",
    "    - They are typically transformed using ordinal encoding, where each category is replaced by its rank (low = 1, medium = 2, high = 3)\n",
    "- Nominal features: reprsent categories with no inherent order. \n",
    "    - example: \"red, blue, green\"\n",
    "    - one-hot encoding, where each cateogry is represented as a binary column (1 indicating presence, 0 indicating no presence)\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "Feature scaling is a method used to normalize the range of independent variables or features of data. It is essential when the data involves features with different units or scales, since many machine learning models are sensitive to the range of input features.\n",
    "\n",
    "There are 2 popular methods for feature scaling\n",
    "- Normalization rescales the values of a feature so that they range between 0 and 1\n",
    "- Standardization is preferred when the feature distribution is Gaussian or when there are outliers, so it doesn't squish the data into a strict 0 to 1 range\n",
    "\n",
    "Sequential Backwards Selection (SBS) is a feature selection technique used to reduce the number of input features while maintaining or improving model performance. The goal is to remove irrelevant or redundant features that do not contribute to the predictive performance of a model. \n",
    "- Basically, train the model\n",
    "- evaluate performance\n",
    "- remove feature that leads to least decrease in model performance\n",
    "- repeat until a stopping criterion is reached (significant drop in performance, etc.)\n",
    "\n",
    "SBS is a greedy algorithm, and does not explore all possible subsets of features but sequentially removes them, which makes it computationally less expensive than exhaustive methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction aims to reduce the number of features (or dimensions) in a dataset while retaining as much information as possible. This helps with simplifying models, reducing computational cost, and mitigating the curse of dimensionality, which can lead to overfitting.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an unsupervised linear dimensionality reduction technique that transforms a datasetâ€™s features into a new set of linearly uncorrelated variables called **principal components (PCs)**. These components are ordered by the amount of variance they capture.\n",
    "\n",
    "- **Variance Maximization**: PCA projects data onto directions (PCs) that maximize variance in the dataset. The first principal component captures the most variance, the second captures the next largest, and so on.\n",
    "  \n",
    "- **Orthogonal Transformation**: The principal components are orthogonal (uncorrelated) to one another, ensuring that each component captures unique variance in the data.\n",
    "\n",
    "- **Linear Method**: PCA assumes the data can be represented in a **linear subspace**. This means it works best when the underlying structure of the data is approximately linear.\n",
    "\n",
    "- **Eigenvectors and Eigenvalues**: PCA relies on the **eigenvectors** and **eigenvalues** of the covariance matrix of the data. The eigenvectors represent the directions (principal components), and the eigenvalues measure the magnitude of variance in each direction.\n",
    "\n",
    "- **Dimensionality Selection**: You can reduce the dataset to fewer dimensions by selecting only the first few principal components that capture the majority of the variance. Typically, the number of components is selected by keeping enough components to explain, say, 95% of the variance.\n",
    "\n",
    "- **Key Equations**:\n",
    "  - Covariance matrix: $ \\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)(x_i - \\mu)^T $\n",
    "  - Eigenvalue decomposition: $ \\Sigma v = \\lambda v $\n",
    "  - Project data: $ X_{\\text{projected}} = X W_k $ (where $ W_k $ is the matrix of top $ k $ eigenvectors)\n",
    "\n",
    "### Supervised Dimensionality Reduction\n",
    "\n",
    "Supervised dimensionality reduction methods consider both the feature set (independent variables) and the target labels (dependent variables) during the reduction process. The goal is to reduce dimensionality while preserving class separability.\n",
    "\n",
    "#### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is a supervised linear technique that aims to maximize the separation between different classes in a lower-dimensional space.\n",
    "\n",
    "- **Maximizing Class Separability**: LDA projects data onto a lower-dimensional space by maximizing the **between-class variance** and minimizing the **within-class variance**. This increases the distance between class means while minimizing the spread of each class.\n",
    "\n",
    "- **Class-Specific Means and Covariance**: LDA calculates the means of each class and the overall covariance matrix to find the optimal directions for projection.\n",
    "\n",
    "- **Linear Combination of Features**: LDA finds a linear combination of features that best separates two or more classes.\n",
    "\n",
    "- **Assumptions**:\n",
    "  - Classes are normally distributed (Gaussian distributions).\n",
    "  - Classes have the same covariance matrix (homoscedasticity).\n",
    "  \n",
    "- **Key Equations**:\n",
    "  - Between-class scatter matrix: $ S_B = \\sum_{i=1}^{k} n_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T $\n",
    "  - Within-class scatter matrix: $ S_W = \\sum_{i=1}^{k} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T $\n",
    "  - LDA projection: Maximize $ \\frac{|S_B|}{|S_W|} $ to find optimal projection directions.\n",
    "\n",
    "### Nonlinear Dimensionality Reduction\n",
    "\n",
    "Linear methods like PCA and LDA assume that the data lies in a linear subspace, which often doesn't hold in real-world applications. Nonlinear dimensionality reduction techniques aim to uncover the underlying low-dimensional manifold on which the high-dimensional data resides. These methods are often used when the data has a complex structure that cannot be captured by linear transformations.\n",
    "\n",
    "#### Common Nonlinear Methods:\n",
    "\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:\n",
    "  - Projects high-dimensional data into 2D or 3D for visualization.\n",
    "  - Focuses on preserving local structure, making similar points stay close in the lower-dimensional space.\n",
    "  \n",
    "- **Isomap**:\n",
    "  - Aims to preserve the **geodesic distances** between points on a nonlinear manifold.\n",
    "  - Combines classical MDS (multidimensional scaling) with nearest-neighbor graphs.\n",
    "\n",
    "- **Kernel PCA**:\n",
    "  - Extends PCA by using the kernel trick to capture **nonlinear relationships** between features.\n",
    "  - Projects data into a higher-dimensional feature space where linear PCA is performed.\n",
    "\n",
    "- **Key Concepts**:\n",
    "  - **Manifold Learning**: Nonlinear techniques assume the data lies on a lower-dimensional manifold embedded in the higher-dimensional space.\n",
    "  - **Geodesic Distance**: The shortest path between two points on a curved surface or manifold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots\n",
    "A residual is the difference between the predicted target and the actual target. In a perfect predictive model, the residuals should all be zero. \n",
    "\n",
    "Good fit: residuals should be randomly distributed with minimal patterns. If there are patterns then that means that the model is underfitting because there is explanatory structure from the data. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
